---
title: "DMI Hands-On 3"
author: "Ariana Murillo Hernandez and Xareni Reyes Soto"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"      
output:
  html_document:
    toc: true
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Exercise 1

[Stamey et al. 1989](https://www.auajournals.org/doi/10.1016/S0022-5347%2817%2941175-X) examined the correlation between the level of prostate-specific antigen (PSA) and a number of clinical measures in men who were about to receive a radical prostatectomy. PSA is a protein that is produced by the prostate gland. The higher a man’s PSA level, the more likely it is that he has prostate cancer.  
Use the [prostate cancer dataset](data/prostate_data.txt), described [here](data/prostate_description.txt),  to train a model that predicts log of prostate-specific antigen. 
The variables are    

- log cancer volume (lcavol)  
- log prostate weight (lweight)  
- age  
- log of the amount of benign prostatic hyperplasia (lbph)   
- seminal vesicle invasion (svi)  
- log of capsular penetration (lcp)  
- Gleason score (gleason)    
- percent of Gleason scores 4 or 5 (pgg45)  

You can ignore column named "train" and do your own data splitting.  
Do not forget to perform feature selection!   
You can use as examples the [Linear Regression Lab](https://hastie.su.domains/ISLR2/Labs/Rmarkdown_Notebooks/Ch3-linreg-lab.html) and the section related to feature selection from  [Lab: Linear Models and Regularization Methods
](https://hastie.su.domains/ISLR2/Labs/Rmarkdown_Notebooks/Ch6-varselect-lab.html) from the book [An Introduction to Statistical Learning](https://www.statlearning.com/).



In this exercise, we need to do linear regression. We have multiple independent variables and one dependent variable, so we will perform Multiple Linear Regression

```{r}
library(dplyr)
library(leaps)
library(caret)
library(car)

# Read the prostate data table 
data_prostate <- read.table("data/prostate_data.txt", sep= "\t", header=TRUE)
# Then delete the first column, so that it is not repeated
data_prostate <- data_prostate[, -1]

# EDA
# 1. Check the data types
# 2. Check for missing values
# 3. Check for duplicate rows
# 4. Statistics summary

# 1. Check the data types
str(data_prostate)

# 2. We check for missing values
missing_counts <- colSums(is.na(data_prostate))
print(missing_counts) # There are no missing values

# 3. Check if there are any duplicate rows in the table
any(duplicated(data_prostate))     # returns FALSE

# 4. Statistics summary
summary(data_prostate)

# Ignore the column 'train' as suggested by the exercise
data_prostate <- data_prostate %>% dplyr::select(-train)

# Split the data intro training and testing sets
set.seed(100) # For reproducibility
train_index <- createDataPartition(data_prostate$lpsa, p = 0.8, list = FALSE)
train_data <- data_prostate[train_index, ]
test_data <- data_prostate[-train_index, ]

# Perform Feature Selection to determine which subset of predictors we should include,
# use Subset Selection
complete_model <- lm(lpsa ~ ., data = train_data)
step_model <- step(complete_model, direction = "both", trace = FALSE) # This is Stepwise Selection

# Info about selected model
summary(step_model) # Stepwise selection kept the most statistically significant predictors, which are: lcavol, lweight, age, lbph and svi. 

# Refit using the predictors selected in previous step
final_model <- lm(lpsa ~ lcavol + lweight + age + lbph + svi, data = train_data)

## Check model assumptions before making predictions on the test data
# Linearity, normality, multicollinearity

# Plot the residuals to check the linearity model assumption
plot(final_model$fitted.values, residuals(final_model), 
     main = "Residual plot",
     xlab = "Fitted values", 
     ylab = "Residuals", 
     pch = 20, col = "darkblue")
abline(h = 0, col = "darkorange")

# Check normality of residuals
qqnorm(residuals(final_model))
qqline(residuals(final_model), col = "#732370")

# Check collinearity
vif(final_model) # A value of VIF (Variance Inflation Factor) greater than 5 indicates a collinearity issue
```

The residual plot looks good, since the points should be randomly scattered around 0, without apparent patterns. Therefore, the linearity assumption is met. Additionally, residuals show a consistent spread (i.e. equal variance).
Furthermore, the residuals are close to the Q-Q line, which indicates that they follow a normal distribution. Also, the VIF values look good, they are just above 1 (value of 1 means they are not correlated, and a value greater than 5 means they are highly correlated).
Therefore, our model assumptions are met and we can proceed to make predictions on the test data.
```{r}
# Predict on test data
predictions <- predict(final_model, test_data)

```

The next step is to evaluate the model. For regression models, we use numeric evaluation metrics, like RMSE, R² or residual analysis.
```{r}
# Evaluate the model
mse <- mean((predictions - test_data$lpsa)^2) # Mean squared error
rmse <- sqrt(mse) # Root mean squared error
r_squared <- cor(predictions, test_data$lpsa)^2 # Computation of R-squared

cat("RMSE:", rmse, "\n")
cat("R-squared:", r_squared, "\n")

```
R^2 = 0.5390 and RMSE = 0.7646, the first value indicates a moderate fit and the second a moderate error. Let us try regularization (Ridge and Lasso) to try to improve these metrics.
```{r}
library(glmnet)
library(caret)

# Prepare data for glmnet, it requires a matrix of predictors and a response vector
# Define predictor variables (exclude lpsa since this is our response variable)
x_train <- model.matrix(lpsa ~ ., data = train_data)[, -1]
x_test <- model.matrix(lpsa ~ ., data = test_data)[, -1]

# Define the response variable
y_train <- train_data$lpsa
y_test <- test_data$lpsa

# Define a sequence of lambda values
lambda_seq <- 10^seq(3, -3, by = -0.1)

## Ridge model
# Use Ridge model, together with cross-validation to find the best lambda
ridge_cv <- cv.glmnet(x_train, y_train, alpha = 0, lambda = lambda_seq)

# Best lambda value
best_lambda_ridge <- ridge_cv$lambda.min
print(best_lambda_ridge)

# Train final Ridge model using the best lambda
ridge_model <- glmnet(x_train, y_train, alpha = 0, lambda = best_lambda_ridge)

# Make predictions on test data
predictions_ridge <- predict(ridge_model, s = best_lambda_ridge, newx = x_test)

## Use Lasso model
# Use Lasso model, together with cross-validation to find the best lambda
lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1, lambda = lambda_seq)

# Best lambda value
best_lambda <- lasso_cv$lambda.min
print(best_lambda)

# Train final Lasso model using the best lambda
lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = best_lambda)

# Make predictions on test data
predictions_lasso <- predict(lasso_model, s = best_lambda, newx = x_test)

# Compute RMSE for Ridge and Lasso
ridge_rmse <- sqrt(mean((predictions_ridge - y_test)^2))
lasso_rmse <- sqrt(mean((predictions_lasso - y_test)^2))

# Compute R^2 for Ridge and Lasso
ridge_r2 <- cor(predictions_ridge, y_test)^2
lasso_r2 <- cor(predictions_lasso, y_test)^2

cat("Ridge RMSE:", ridge_rmse, "\n")
cat("Lasso RMSE:", lasso_rmse, "\n")
cat("Ridge R²:", ridge_r2, "\n")
cat("Lasso R²:", lasso_r2, "\n")


```

Previously, our model explained 0.5390 of the variance, and with regularization we see an improvement in both cases (we can explain more variance after regularization). Also, the values for RMSE decreased in both cases, which means our model's performance improved after applying regularization.

# Exercise 2

Use the [breast cancer dataset](data/breat_cancer_data.csv) to train a model that predicts whether a future tumor image (with unknown diagnosis) is a benign or malignant tumor. Try different machine learning algorithms such as:   
- KNNs  
- Decision trees  
- Random forest  
- Logistic Regression  

The breast cancer dataset contains digitized breast cancer image features, and was created by [Dr. William H. Wolberg, W. Nick Street, and Olvi L. Mangasarian](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29). Each row in the data set represents an image of a tumor sample, including the diagnosis (benign or malignant) and several other measurements (nucleus texture, perimeter, area, and more). Diagnosis for each image was conducted by physicians.

Do not forget to perform hyperparameter tuning!   
Which of all models performs better for this data? Discuss.  

Generate a ROC curve for all the models. 

You can use as a guide the analysis of this dataset included in the [chapter 5](https://datasciencebook.ca/classification1.html) of the Data Science, A First Introduction Book.
Additionally, for further information and ideas, you can check [this post](https://www.rebeccabarter.com/blog/2020-03-25_machine_learning/)

```{r}
library(dplyr)
library(caret)
library(class)
library(pROC)

data_breast_cancer <- read.csv("data/breat_cancer_data.csv", sep= ",", header=TRUE)

## Exploratory Data Analysis
# 1. Check the data types
str(data_breast_cancer)  # The data types look correct: id has type integer, diagnosis has type char and all the rest have type num 

# 2. Check if there are missing values
missing_counts <- colSums(is.na(data_breast_cancer))
print(missing_counts)    # Only column "X" has 569 NA's, no missing values in the other columns

# 3. Check unique values
unique(data_breast_cancer$diagnosis) # There are only two values: "M" (malign) and "B" (benign)

# 4. Check if there are any duplicate rows in the table
any(duplicated(data_breast_cancer))     # returns FALSE

# 5. Statistics summary
summary(data_breast_cancer)

# 6. Look at the distribution of the target variable: diagnosis
num_counts <- as.data.frame(table(data_breast_cancer$diagnosis)) # There are 357 benign samples and 212 malign samples

## Data pre-processing
# Drop unnecessary columns (the last one)
data_breast_cancer <- data_breast_cancer %>% select(-id, -X)

# Select only the diagnosis column and the next 10 columns (containing "..._mean")
selected_columns <- c("diagnosis", "radius_mean", "texture_mean","perimeter_mean", "area_mean", "smoothness_mean","compactness_mean", "concavity_mean","concave.points_mean", "symmetry_mean", "fractal_dimension_mean")
data_selected <- data_breast_cancer[selected_columns]

# Convert the categorical variable "diagnosis" to the factor data type for more efficiency in memory use and better performance in ML classification models
data_selected$diagnosis <- factor(data_selected$diagnosis)
# Verify the conversion to factor is done correctly
str(data_selected) # The column "diagnosis" appears now as Factor

# Normalize the data: this step is crucial for the KNN algorithm
preProcessValues <- preProcess(data_selected[-1], method = c("center","scale"))
data_normalized<- predict(preProcessValues, data_selected[-1])

data_final <- cbind(diagnosis = data_selected$diagnosis, data_normalized)

# Split the data into training and testing sets
set.seed(100)
trainIndex <- createDataPartition(data_final$diagnosis, p = 0.8, list = FALSE)
train_data <- data_final[trainIndex, ] # 80% of the data for training
test_data <- data_final[-trainIndex, ] # 20% of the data for testing

```
Now that we have explored and pre-processed the data, we proceed with the KNN algorithm.
```{r}
## KNN algorithm
# The key hyperparameter for this model is K, the number of neighbors to consider.
# Use a 10-fold cross-validation instead of a single split to choose the best value of K (i.e. tune the hyperparameter K) 

# Define tuning "grid" for KNN (odd values from 1 to 20)
tune_k_values<- expand.grid(k = seq(1, 20, by = 2))

# Set up cross-validation
control <- trainControl(method = "cv", number = 10) # 10-fold cross-validation

# Train KNN model with hyperparameter tuning
knn_model <- train(diagnosis ~ ., data = train_data, method = "knn",
                   trControl= control, tuneGrid = tune_k_values)

# Print the best value for K 
print(knn_model$bestTune) # The best value for K is 15

# Predict on test data. Use probability output for a smoother ROC curve
predictions_knn <- predict(knn_model, newdata = test_data, type = "prob")

# Convert probabilities into diagnosis labels
predicted_classes_knn <- ifelse(predictions_knn[,2] > 0.5, "M", "B") # Take 0.5 as threshold 
predicted_classes_knn <- factor(predicted_classes_knn, levels = levels(test_data$diagnosis))

# Evaluate the model with a confusion matrix
# A confusion matrix expects factors, not probabilities
confusion_matrix_knn <- confusionMatrix(predicted_classes_knn, test_data$diagnosis)
print(confusion_matrix_knn)

# Generate ROC curve for KNN
roc_curve_knn <- roc(as.numeric(test_data$diagnosis)-1, predictions_knn[,2])
plot(roc_curve_knn, col = "#ca3517", main = "ROC Curve for the KNN model") 
auc(roc_curve_knn) # The area under the ROC curve is 0.9866

```
We will next use a decision tree as model. We have already have done an important part of the work, since we have explored, cleaned, normalized and split the data.
```{r}
library(rpart)
library(rpart.plot)

## Decision tree
# The hyperparameter we will tune in this case is cp (complexity parameter)

# Define tuning "grid" for Decision Tree
tune_grid_tree <- expand.grid(cp = seq(0.001, 0.05, by = 0.002))

# Set up cross-validation
control_dtree <- trainControl(method = "cv", number = 10) # 10-fold cross-validation

# Train Decision Tree model with hyperparameter tuning
decision_tree_model <- train(diagnosis ~ ., data = train_data, method = "rpart",
                   trControl= control_dtree, tuneGrid = tune_grid_tree)

# Print the best value for cp
print(decision_tree_model$bestTune) # The best value for cp is 0.013

# Visualize the optimized Decision Tree
rpart.plot(decision_tree_model$finalModel, main = "Optimized Decision Tree for the Diagnosis of Breast Cancer" )

# Make predictions on test_data.  Use probability output for a smoother ROC curve
predictions_dtree <- predict(decision_tree_model, test_data[-1], type = "prob")

# Convert probabilities into diagnosis labels
predicted_classes_dtree <- ifelse(predictions_dtree[,2] > 0.5, "M", "B")
predicted_classes_dtree <- factor(predicted_classes_dtree, levels = levels(test_data$diagnosis))

# Evaluate the model
confusion_matrix_dtree <- confusionMatrix(predicted_classes_dtree, test_data$diagnosis)
print(confusion_matrix_dtree)

# Generate ROC Curve for Decision Tree
roc_curve_dt <- roc(as.numeric(test_data$diagnosis)-1, predictions_dtree[,2])
plot(roc_curve_dt, col = "#063970", main = "ROC Curve for the Decision Tree model")
auc(roc_curve_dt) # The area under the ROC curve is 0.9489

```
We will next use logistic regression as model. The same remark about exploratory data analysis and data pre-processing applies as for decision tree.
```{r}
## Logistic regression
library(glmnet)

# The main hyperparameter for logistic regression is regularization, we use Ridge regression
# Convert labels ("M" and "B") to binary
train_data$diagnosis <- factor(ifelse(train_data$diagnosis == "M", 1, 0), levels = c(0,1))
test_data$diagnosis <- factor(ifelse(test_data$diagnosis == "M", 1, 0), levels = c(0,1))

# Prepare data for glmnet
x_train <-as.matrix(train_data[, -1])
x_test <- as.matrix(test_data[, -1])

# Define the response variable
y_train <- train_data$diagnosis
y_test <- test_data$diagnosis

# Perform hyperparameter tuning with cross-validation
set.seed(100)
cv_logistic <- cv.glmnet(x_train, as.numeric(y_train) - 1, family = "binomial", alpha = 0)

# Best lambda value
best_lambda_logistic <- cv_logistic$lambda.min
print(best_lambda_logistic)

# Train final Logistic Regression model using the best lambda
logistic_model_tuned <- glmnet(x_train, as.numeric(y_train) - 1, family = "binomial", alpha = 0, lambda = best_lambda_logistic)

# Make predictions on test data
predictions_log_reg_tuned <- predict(logistic_model_tuned, x_test, type = "response")

# Convert probabilities into diagnosis labels
predicted_classes_log_reg_tuned <- factor(ifelse(predictions_log_reg_tuned > 0.5, 1, 0), levels = c(0,1))

# Important: convert both predictions and actual values to factors with the same levels
y_test <- factor(y_test, levels = c(0, 1))

# Check if levels are the same
print(levels(predicted_classes_log_reg_tuned))
print(levels(y_test))

# Evaluate the model
confusion_matrix_log_reg_tuned <- confusionMatrix(predicted_classes_log_reg_tuned, y_test, positive = "1")
print(confusion_matrix_log_reg_tuned)

# Generate ROC Curve 
roc_curve_log_reg_tuned <- roc(as.numeric(y_test), as.numeric(predictions_log_reg_tuned))
plot(roc_curve_log_reg_tuned, col = "#036169", main = "ROC Curve for the Logistic Regression model tuned")
auc(roc_curve_log_reg_tuned) # The area under the ROC curve is 

```

To determine which model performs the best in this specific case, we look at the confusion matrices and AUC values for KNN, Decision Tree and Logistic Regression.
Both KNN and Logistic regression achieve the highest AUC (0.9866), indicating a better classification performance than Decision Tree. Additionally, KNN has the highest Balanced Accuracy (0.9410), followed by Logistic Regression (0.9291) and Decision Tree (0.9199). Given that Decision Tree has the lowest AUC and Balanced Accuracy, we conclude it is the least robust model in this case. 

Both KNN and Logistic Regression models perform well, but we believe Logistic Regression is the best choice because it outputs probabilities instead of just categorical predictions. These probability scores provide more meaningful clinical insights.

# Exercise 3  

Use [The Cancer Genome Atlas (TCGA)](https://www.genome.gov/Funded-Programs-Projects/Cancer-Genome-Atlas) gene expression data of two different cancer types to build a machine learning model that identifies whether a given sample (gene counts) belongs to one or the other. The TCGA is a comprehensive and coordinated effort to accelerate our understanding of the molecular basis of cancer through the application of genome analysis technologies, including large-scale genome sequencing. The program has generated, analyzed, and made available genomic sequence, expression, methylation, and copy number variation data on over 11,000 individuals who represent over 30 different types of cancer. 
After building your model, you should predict the cancer types for [10 unkwnon samples](data/unknwown_samples.tsv).  

For this task, you should retrieve the TCGA data from the [Genomic Data Commons Data Portal](https://portal.gdc.cancer.gov/). If necessary you can watch [the video uploaded in the Campus Global](https://aulaglobal.upf.edu/mod/resource/view.php?id=1483539). The video assumes that you have previously installed the [GDC data transfer tool](https://gdc.cancer.gov/access-data/gdc-data-transfer-tool). 

Each team will work with two specific cancer types, that will be assigned in class.

Important notice: if you do not have a lot of hard drive space in your laptop, you can modify the manifest file to download only 50 samples per cancer types. 

```{r}

#Load libraries
library(here)       # To manage relative paths
library(dplyr)      # Data manipulation
library(tidymodels) # Modeling and tuning
library(glmnet)
library(pROC)
library(class)
library(caret)
library(rpart)
library(rpart.plot)
library(kknn)
library(ranger)
library(randomForest)
library(ggplot2)
```


```{r}
# Define the base folder using relative paths
base_path_LAML <- here("data", "LAML100")
base_path_BLCA <- here("data", "BLCA100")
unknown_data_path <- here("data", "unknown_samples.tsv")

# Obtain the list of .tsv files inside subfolders
LAML_files <- list.files(base_path_LAML, pattern = "\\.tsv$", recursive = TRUE, full.names = TRUE)
BLCA_files <- list.files(base_path_BLCA, pattern = "\\.tsv$", recursive = TRUE, full.names = TRUE)

# Function to read and process files
process_files <- function(files) {
  dataset <- data.frame()
  
  for (i in seq_along(files)) {
    file <- files[i]
    
    if (file.exists(file)) {
      df <- read.table(file, header = TRUE, skip = 1, sep = "\t")
      
      # Ensure there are enough rows
      if (nrow(df) > 4) {
        df <- df[-c(1:4), ]
      }
      
      # Select columns of interest if available
      if ("gene_id" %in% colnames(df) && "tpm_unstranded" %in% colnames(df)) {
        df_subset <- df[, c("gene_id", "tpm_unstranded")]
        
        # Merge datasets
        dataset <- if (i == 1) df_subset else full_join(dataset, df_subset, by = "gene_id")
      }
    } else {
      warning(paste("File not found:", file))
    }
  }
  
  return(dataset)
}

# Process LAML and BLCA datasets
LAML <- process_files(LAML_files)
BLCA <- process_files(BLCA_files)

# Transpose the datasets
LAML <- data.frame(t(LAML))
BLCA <- data.frame(t(BLCA))

# Set column names and row names
colnames(LAML) <- LAML[1,]
LAML <- LAML[-1,]
rownames(LAML) <- paste0("LAML_", 1:nrow(LAML))

colnames(BLCA) <- BLCA[1,]
BLCA <- BLCA[-1,]
rownames(BLCA) <- paste0("BLCA_", 1:nrow(BLCA))

# Merge the datasets
merge_data <- rbind(LAML, BLCA)
merge_data <- merge_data %>% mutate(type = ifelse(grepl("^LAML", rownames(merge_data)), "LAML", "BLCA"))

# Load unknown data
if (file.exists(unknown_data_path)) {
  unknown_data <- read.table(unknown_data_path, header = TRUE, sep = "\t")
  unknown_data <- t(unknown_data)
  colnames(unknown_data) <- unknown_data[1,]
  unknown_data <- unknown_data[-c(1,2),]
  unknown_data <- as.data.frame(unknown_data)
} else {
  warning(paste("Unknown data file not found:", unknown_data_path))
}

# Show a preview of the combined data
#head(merge_data)

```

```{r}
 # RANDOM FOREST ---------------------------------------------------------

# Convert datasets to numeric
LAML <- lapply(LAML, as.numeric)
BLCA <- lapply(BLCA, as.numeric)
LAML <- as.data.frame(LAML)
BLCA <- as.data.frame(BLCA)

# Calculate absolute differences in gene expression
expression_difference <- abs(apply(LAML, 2, mean) - apply(BLCA, 2, mean))

# Sort genes by difference
sorted_genes <- sort(expression_difference, decreasing = TRUE)

# Select top 50 genes
top50genes <- names(head(sorted_genes, 50))

# Filter dataset
filtered <- merge_data[, c(top50genes, "type"), drop = FALSE]

# Train-test split
set.seed(2024)
training_indices <- sample(1:nrow(filtered), 0.7 * nrow(filtered))
test_indices <- setdiff(1:nrow(filtered), training_indices)

train_data <- filtered[training_indices, ]
test_data <- filtered[test_indices, ]

# Convert variables to correct types
train_data[, -ncol(train_data)] <- lapply(train_data[, -ncol(train_data)], as.numeric)
train_data[[ncol(train_data)]] <- as.factor(train_data[[ncol(train_data)]])

test_data[, -ncol(test_data)] <- lapply(test_data[, -ncol(test_data)], as.numeric)
test_data[[ncol(test_data)]] <- as.factor(test_data[[ncol(test_data)]])

# Define recipe
cancer_recipe <- recipe(type ~ ., data = train_data)

# Define Random Forest model
cancer_model <- rand_forest() %>%
  set_args(mtry = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

# Define tuning grid
cancer_grid <- expand.grid(mtry = c(5:15))

# Create workflow
rf_workflow <- workflow() %>%
  add_recipe(cancer_recipe) %>%
  add_model(cancer_model)

# Cross-validation
training_cv <- vfold_cv(train_data)

# Tune hyperparameters
set.seed(2024)
cancer_tune <- tune_grid(rf_workflow, resamples = training_cv, grid = cancer_grid)

# Select the best model
best_cancer <- select_best(cancer_tune, metric = "accuracy")
final_cancer <- finalize_workflow(rf_workflow, best_cancer)

# Train the final model
cancer_fit <- final_cancer %>% fit(train_data)

# Predictions
predictions_cancer <- predict(cancer_fit, test_data, type = "class")

# Calculate accuracy
cancer_accuracy <- mean(predictions_cancer$.pred_class == test_data$type)
print(paste("Accuracy Random Forest:", cancer_accuracy))

# ROC Curve
cancer_roc <- roc(test_data$type, as.numeric(predictions_cancer$.pred_class))
auc_value <- auc(cancer_roc)
cat("Area under the curve (AUC):", auc_value, "\n")

# Plot ROC curve
plot(cancer_roc, main = "ROC Curve - Random Forest",
     col = "#E63946", lwd = 3, print.auc = TRUE,
     auc.polygon = TRUE, auc.polygon.col = rgb(1, 0, 0, alpha = 0.2))
grid(col = "gray70")

```

The test set prediction accuracy reached 100%, indicating a high likelihood of overfitting. Despite efforts to mitigate this by reducing tree depth, increasing the number of selected genes, and expanding the number of trees, overfitting persisted. Utilizing the full dataset along with all available genes could potentially help alleviate this issue.

```{r}
# PREDICTIONS FOR UNKNOWN SAMPLES ---------------------------------------------

# Ensure unknown dataset contains the same top 50 genes
filtered_unknown <- unknown_data[, top50genes, drop = FALSE]
filtered_unknown <- mutate_all(filtered_unknown, as.numeric)

# Get predictions and probabilities
predictions_uk <- predict(cancer_fit, filtered_unknown, type = "prob")
predictions_uk <- as.data.frame(predictions_uk)

# Extract predicted class and probability
predicted_class <- colnames(predictions_uk)[max.col(predictions_uk, ties.method = "first")]
max_probability <- apply(predictions_uk, 1, max)

# Create results table
result_table <- data.frame(
  Individual = rownames(filtered_unknown),
  Cancer_Class_Prediction = gsub('.pred_', '', predicted_class),
  Prediction_Probability = max_probability
)

# Print results
print(result_table)

# Visualization: Prediction Probabilities
ggplot(result_table, aes(x = Cancer_Class_Prediction, y = Prediction_Probability, fill = Cancer_Class_Prediction)) +
  geom_boxplot(alpha = 0.6, outlier.shape = NA) +
  geom_jitter(width = 0.2, alpha = 0.8, size = 2) +
  labs(title = "Prediction Probabilities for Unknown Samples",
       x = "Predicted Cancer Type", y = "Prediction Probability") +
  theme_minimal() +
  scale_fill_manual(values = c("#1f78b4", "#e31a1c"))

# Count predictions above threshold
threshold <- 0.9
high_confidence_count <- sum(result_table$Prediction_Probability >= threshold)
cat("We have", high_confidence_count, "observations with a probability higher than", threshold, ".\n")

```
We have 4 observations with a probability higher than 0.9. 


# session info {.unnumbered}

```{r, results='asis',  echo=FALSE, message=FALSE }
sessionInfo()
```
